from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# Import xgboost
import xgboost as xgb
# import the mean_squared_error from sklearn library
from sklearn.metrics import mean_squared_error
dataset = pd.read_csv('../../datasets/ames_housing_trimmed_processed.csv')
dataset.head() 
X, y = dataset.iloc[:,:-1].values, dataset.iloc[:,-1].values
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)
# Instantiate the XGBRegressor as xg_reg
xg_reg = xgb.XGBRegressor(seed=123, objective="reg:linear", n_estimators=10, booster="gbtree")
xg_reg.fit(X_train,y_train)
preds = xg_reg.predict(X_test)
mse = mean_squared_error(y_test, preds)
rmse = np.sqrt(mse)
print("RMSE: %f" % (rmse))
DM_train = xgb.DMatrix(data=X_train, label=y_train)
DM_test =  xgb.DMatrix(data=X_test, label=y_test)
params = {'objective':'reg:linear', 'booster':'gbtree'}
xg_reg = xgb.train(params=params, dtrain=DM_train, num_boost_round=5)
preds = xg_reg.predict(DM_test)
rmse = np.sqrt(mean_squared_error(y_test, preds))
print("RMSE: %f" % (rmse))
housing_dmatrix = xgb.DMatrix(data=X, label=y)
params = {'objective':'reg:linear', "max_depth":4}
cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, as_pandas=True, metrics="rmse", seed=123)
print(cv_results)
print((cv_results.iloc[-1,0]))
housing_dmatrix = xgb.DMatrix(data=X, label=y)
reg_params = [1, 10, 100]
params = {'objective':'reg:linear', 'max_depth':3}
rmses_l2 = []
for reg in reg_params:
    
    # Update l2 strength
    params["lambda"] = reg
    
    # Pass this updated param dictionary into cv
    cv_results_rmse =  xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, as_pandas=True, metrics="rmse", seed=123)
    
    # Append best rmse (final round) to rmses_l2
    rmses_l2.append(cv_results_rmse["test-rmse-mean"].tail(1).values[0])
    print("Best rmse as a function of l2:")
print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=["l2", "rmse"]))
params = {'objective':'reg:linear', 'max_depth': 2}
xg_reg = xgb.train(params=params, dtrain=DM_train, num_boost_round=10)
import graphviz
xgb.plot_tree(xg_reg, num_trees=0)
fig = plt.gcf()
fig.set_dpi(100)
fig.set_size_inches(10, 5)
plt.show()
xgb.plot_tree(xg_reg, num_trees=4)
fig = plt.gcf()
fig.set_dpi(100)
fig.set_size_inches(10, 5)
plt.show()
xgb.plot_tree(xg_reg, num_trees=9, rankdir="LR")
fig = plt.gcf()
fig.set_dpi(100)
fig.set_size_inches(10, 5)
plt.show()
params = {'objective':'reg:linear', 'max_depth': 4}
xg_reg = xgb.train(params=params, dtrain=DM_train, num_boost_round=10)
xgb.plot_importance(xg_reg)
fig = plt.gcf()
fig.set_dpi(200)
fig.set_size_inches(5, 3)
plt.show()
